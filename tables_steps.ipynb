{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from pytesseract import Output\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for our random number generator\n",
    "np.random.seed(42)\n",
    "# load the input image and convert it to grayscale\n",
    "path =r'/home/pop-jaroso/Desktop/Table detection/Files/page0.jpg'\n",
    "image = cv2.imread(path)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a rectangular kernel that is ~5x wider than it is tall,\n",
    "# then smooth the image using a 3x3 Gaussian blur and then apply a\n",
    "# blackhat morphological operator to find dark regions on a light\n",
    "# background\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (51, 11))\n",
    "gray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the Scharr gradient of the blackhat image and scale the\n",
    "# result into the range [0, 255]\n",
    "grad = cv2.Sobel(blackhat, ddepth=cv2.CV_32F, dx=1, dy=0, ksize=-1)\n",
    "grad = np.absolute(grad)\n",
    "(minVal, maxVal) = (np.min(grad), np.max(grad))\n",
    "grad = (grad - minVal) / (maxVal - minVal)\n",
    "grad = (grad * 255).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a closing operation using the rectangular kernel to close\n",
    "# gaps in between characters, apply Otsu's thresholding method, and\n",
    "# finally a dilation operation to enlarge foreground regions\n",
    "grad = cv2.morphologyEx(grad, cv2.MORPH_CLOSE, kernel)\n",
    "thresh = cv2.threshold(grad, 0, 255,\n",
    "\tcv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "thresh = cv2.dilate(thresh, None, iterations=3)\n",
    "#cv2.imshow(\"Thresh\", thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find contours in the thresholded image and grab the largest\n",
    "# which we will assume is the stats table\n",
    "cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n",
    "\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = imutils.grab_contours(cnts)\n",
    "tableCnt = max(cnts, key=cv2.contourArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the bounding box coordinates of the stats table and extract\n",
    "# the table from the input image\n",
    "(x, y, w, h) = cv2.boundingRect(tableCnt)\n",
    "table = image[y:y + h, x:x + w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the original input image and extracted table to our screen\n",
    "#cv2.imshow(\"Input\", image)\n",
    "#cv2.imshow(\"Table\", table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the PSM mode to detect sparse text, and then localize text in\n",
    "# the table\n",
    "options = \"--psm 6\"\n",
    "results = pytesseract.image_to_data(\n",
    "\tcv2.cvtColor(table, cv2.COLOR_BGR2RGB),\n",
    "\tconfig=options,\n",
    "\toutput_type=Output.DICT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a list to store the (x, y)-coordinates of the detected\n",
    "# text along with the OCR'd text itself\n",
    "coords = []\n",
    "ocrText = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over each of the individual text localizations\n",
    "for i in range(0, len(results[\"text\"])):\n",
    "\t# extract the bounding box coordinates of the text region from\n",
    "\t# the current result\n",
    "\tx = results[\"left\"][i]\n",
    "\ty = results[\"top\"][i]\n",
    "\tw = results[\"width\"][i]\n",
    "\th = results[\"height\"][i]\n",
    "\t# extract the OCR text itself along with the confidence of the\n",
    "\t# text localization\n",
    "\ttext = results[\"text\"][i]\n",
    "\tconf = int(results[\"conf\"][i])\n",
    "\t# filter out weak confidence text localizations\n",
    "\tif conf > 0: #args[\"min_conf\"]\n",
    "\t\t# update our text bounding box coordinates and OCR'd text,\n",
    "\t\t# respectively\n",
    "\t\tcoords.append((x, y, w, h))\n",
    "\t\tocrText.append(text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AgglomerativeClustering(affinity=&#x27;manhattan&#x27;, distance_threshold=25.0,\n",
       "                        linkage=&#x27;complete&#x27;, n_clusters=None)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AgglomerativeClustering</label><div class=\"sk-toggleable__content\"><pre>AgglomerativeClustering(affinity=&#x27;manhattan&#x27;, distance_threshold=25.0,\n",
       "                        linkage=&#x27;complete&#x27;, n_clusters=None)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AgglomerativeClustering(affinity='manhattan', distance_threshold=25.0,\n",
       "                        linkage='complete', n_clusters=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract all x-coordinates from the text bounding boxes, setting the\n",
    "# y-coordinate value to zero\n",
    "xCoords = [(c[0], 0) for c in coords]\n",
    "# apply hierarchical agglomerative clustering to the coordinates\n",
    "clustering = AgglomerativeClustering(\n",
    "\tn_clusters=None,\n",
    "\taffinity=\"manhattan\",\n",
    "\tlinkage=\"complete\",\n",
    "\tdistance_threshold= 25.0) #args[\"dist_thresh\"]\n",
    "clustering.fit(xCoords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our list of sorted clusters\n",
    "sortedClusters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all clusters\n",
    "for l in np.unique(clustering.labels_):\n",
    "\t# extract the indexes for the coordinates belonging to the\n",
    "\t# current cluster\n",
    "\tidxs = np.where(clustering.labels_ == l)[0]\n",
    "\t# verify that the cluster is sufficiently large\n",
    "\tif len(idxs) > 2: #args[\"min_size\"]\n",
    "\t\t# compute the average x-coordinate value of the cluster and\n",
    "\t\t# update our clusters list with the current label and the\n",
    "\t\t# average x-coordinate\n",
    "\t\tavg = np.average([coords[i][0] for i in idxs])\n",
    "\t\tsortedClusters.append((l, avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the clusters by their average x-coordinate and initialize our\n",
    "# data frame\n",
    "sortedClusters.sort(key=lambda x: x[1])\n",
    "df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the clusters again, this time in sorted order\n",
    "for (l, _) in sortedClusters:\n",
    "\t# extract the indexes for the coordinates belonging to the\n",
    "\t# current cluster\n",
    "\tidxs = np.where(clustering.labels_ == l)[0]\n",
    "\t# extract the y-coordinates from the elements in the current\n",
    "\t# cluster, then sort them from top-to-bottom\n",
    "\tyCoords = [coords[i][1] for i in idxs]\n",
    "\tsortedIdxs = idxs[np.argsort(yCoords)]\n",
    "\t# generate a random color for the cluster\n",
    "\tcolor = np.random.randint(0, 255, size=(3,), dtype=\"int\")\n",
    "\tcolor = [int(c) for c in color]\n",
    "\n",
    "    \t# loop over the sorted indexes\n",
    "\tfor i in sortedIdxs:\n",
    "\t\t# extract the text bounding box coordinates and draw the\n",
    "\t\t# bounding box surrounding the current element\n",
    "\t\t(x, y, w, h) = coords[i]\n",
    "\t\tcv2.rectangle(table, (x, y), (x + w, y + h), color, 2)\n",
    "\t# extract the OCR'd text for the current column, then construct\n",
    "\t# a data frame for the data where the first entry in our column\n",
    "\t# serves as the header\n",
    "\tcols = [ocrText[i].strip() for i in sortedIdxs]\n",
    "\tcurrentDF = pd.DataFrame({cols[0]: cols[1:]})\n",
    "\t# concatenate *original* data frame with the *current* data\n",
    "\t# frame (we do this to handle columns that may have a varying\n",
    "\t# number of rows)\n",
    "\tdf = pd.concat([df, currentDF], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] saving CSV file to disk...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace NaN values with an empty string and then show a nicely\n",
    "# formatted version of our multi-column OCR'd text\n",
    "df.fillna(\"\", inplace=True)\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n",
    "# write our table to disk as a CSV file\n",
    "print(\"[INFO] saving CSV file to disk...\")\n",
    "df.to_csv('datos.csv', index=False) #args[\"output\"]\n",
    "# show the output image after performing multi-column OCR\n",
    "#cv2.imshow(\"Output\", image)\n",
    "cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tables')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e09d13da78c53901cd78978d36c39c81fe2a4bab98b3efffff9436854202793"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
